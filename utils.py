import osimport copyimport numpy as npimport torchfrom torch.utils.data import Datasetfrom transformers import AutoTokenizerimport csvdef pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):    x = (np.ones(maxlen) * value).astype(dtype)    if truncating == 'pre':        trunc = sequence[-maxlen:]    else:        trunc = sequence[:maxlen]    trunc = np.asarray(trunc, dtype=dtype)    if padding == 'post':        x[:len(trunc)] = trunc    else:        x[-len(trunc):] = trunc    return xclass Tokenizer4Bert:    def __init__(self, max_seq_len, pretrained_bert_name, model_type):        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_name)        self.max_seq_len = max_seq_len    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))        if len(sequence) == 0:            sequence = [0]        if reverse:            sequence = sequence[::-1]        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)    def id_to_sequence(self, sequence, reverse=False, padding='post', truncating='post'):        if len(sequence) == 0:            sequence = [0]        if reverse:            sequence = sequence[::-1]        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)class DepInstanceParser():    def __init__(self, basicDependencies, tokens):        self.basicDependencies = basicDependencies        self.tokens = tokens        self.words = []        self.dep_governed_info = []        self.dep_parsing()    def dep_parsing(self):        if len(self.tokens) > 0:            words = []            for token in self.tokens:                token['word'] = token                words.append(self.change_word(token['word']))            dep_governed_info = [                {"word": word}                for i, word in enumerate(words)            ]            self.words = words        else:            dep_governed_info = [{}] * len(self.basicDependencies)        for dep in self.basicDependencies:            dependent_index = dep['dependent'] - 1            governed_index = dep['governor'] - 1            dep_governed_info[dependent_index] = {                "governor": governed_index,                "dep": dep['dep']            }        self.dep_governed_info = dep_governed_info    def change_word(self, word):        if "-RRB-" in word:            return word.replace("-RRB-", ")")        if "-LRB-" in word:            return word.replace("-LRB-", "(")        return word    def get_first_order(self, direct=False):        dep_adj_matrix = [[0] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]        dep_type_matrix = [["none"] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]        for i, dep_info in enumerate(self.dep_governed_info):            governor = dep_info["governor"]            dep_type = dep_info["dep"]            dep_adj_matrix[i][governor] = 1            dep_adj_matrix[governor][i] = 1            dep_type_matrix[i][governor] = dep_type if direct is False else "{}_in".format(dep_type)            dep_type_matrix[governor][i] = dep_type if direct is False else "{}_out".format(dep_type)        return dep_adj_matrix, dep_type_matrix    def get_next_order(self, dep_adj_matrix, dep_type_matrix):        new_dep_adj_matrix = copy.deepcopy(dep_adj_matrix)        new_dep_type_matrix = copy.deepcopy(dep_type_matrix)        for target_index in range(len(dep_adj_matrix)):            for first_order_index in range(len(dep_adj_matrix[target_index])):                if dep_adj_matrix[target_index][first_order_index] == 0:                    continue                for second_order_index in range(len(dep_adj_matrix[first_order_index])):                    if dep_adj_matrix[first_order_index][second_order_index] == 0:                        continue                    if second_order_index == target_index:                        continue                    if new_dep_adj_matrix[target_index][second_order_index] == 1:                        continue                    new_dep_adj_matrix[target_index][second_order_index] = 1                    new_dep_type_matrix[target_index][second_order_index] = dep_type_matrix[first_order_index][                        second_order_index]        return new_dep_adj_matrix, new_dep_type_matrix    def get_second_order(self, direct=False):        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)        return self.get_next_order(dep_adj_matrix, dep_type_matrix)    def get_third_order(self, direct=False):        dep_adj_matrix, dep_type_matrix = self.get_second_order(direct=direct)        return self.get_next_order(dep_adj_matrix, dep_type_matrix)    def search_dep_path(self, start_idx, end_idx, adj_max, dep_path_arr):        for next_id in range(len(adj_max[start_idx])):            if next_id in dep_path_arr or adj_max[start_idx][next_id] in ["none"]:                continue            if next_id == end_idx:                return 1, dep_path_arr + [next_id]            stat, dep_arr = self.search_dep_path(next_id, end_idx, adj_max, dep_path_arr + [next_id])            if stat == 1:                return stat, dep_arr        return 0, []    def get_dep_path(self, start_index, end_index, direct=False):        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)        _, dep_path = self.search_dep_path(start_index, end_index, dep_type_matrix, [start_index])        return dep_pathclass ABSADataset(Dataset):    def __init__(self, datafile, tokenizer, opt):        self.datafile = datafile        self.depfile = "{}.dep".format(self.datafile)        self.tokenizer = tokenizer        self.opt = opt        self.max_key_len = opt.max_seq_len        self.task = opt.task_name        self.model_type = opt.model_type        self.a_textdata = ABSADataset.load_datafile(self.datafile, self.task, True)  # 用于分别读取句子A和相似性分数        self.a_depinfo = ABSADataset.load_depfile(self.depfile, True) if self.task not in [            'SST'] else ABSADataset.load_alldepfile(self.depfile)        self.b_textdata = ABSADataset.load_datafile(self.datafile, self.task, False)  # 用于分别读取句子B和相似性分数        self.b_depinfo = ABSADataset.load_depfile(self.depfile, False) if self.task not in [            'SST'] else ABSADataset.load_alldepfile(self.depfile)        self.label2id = self.get_label2id(opt)        self.feature = []        for sent_a, dep_a, sent_b, dep_b in zip(self.a_textdata, self.a_depinfo, self.b_textdata,                                                self.b_depinfo):  # 同时读入句子A和B,以及label            self.feature.append([self.create_feature(sent_a, dep_a, self.task, True, self.model_type),                                 self.create_feature(sent_b, dep_b, self.task, False, self.model_type)])        print(self.feature[:1])    def __getitem__(self, index):        return self.feature[index]    def __len__(self):        return len(self.feature)    def ws(self, text):        tokens = []        valid_ids = []        for i, word in enumerate(text):            if len(text) <= 0:                continue            token = self.tokenizer.tokenizer.tokenize(word)            tokens.extend(token)            for m in range(len(token)):                if m == 0:                    valid_ids.append(1)                else:                    valid_ids.append(0)        token_ids = self.tokenizer.tokenizer.convert_tokens_to_ids(tokens)        return tokens, token_ids, valid_ids    def normlized_adj(self, adj_matrix, symmetric=True):        size = len(adj_matrix[0])        A = adj_matrix + np.eye(size)        col_sum = np.sum(A, axis=1)        d_matrix = np.eye(size)        if symmetric:            for i in range(size):                d_matrix[i][i] = np.power(col_sum[i], -0.5)            D = d_matrix            return np.matmul(np.matmul(D, A), D)        else:            for i in range(size):                d_matrix[i][i] = np.power(col_sum[i], -1)            D = d_matrix            return np.matmul(D, A)    def create_feature(self, sentence, depinfo, task, is_senta=True, model_type='bert'):        sent, sim_score = sentence        cls_id = 0  # roberta的编码        sep_id = 2        if model_type == 'bert':            cls_id = self.tokenizer.tokenizer.vocab["[CLS]"]            sep_id = self.tokenizer.tokenizer.vocab["[SEP]"]        elif model_type == 'roberta':            cls_id = self.tokenizer.tokenizer.vocab["<s>"]            sep_id = self.tokenizer.tokenizer.vocab["</s>"]        tokens, token_ids, valid_ids = self.ws(sent.split(" "))        sent_inputs = self.tokenizer.tokenizer(sent, max_length=self.max_key_len, padding=True, truncation=True,                                               return_tensors="pt")        input_ids = sent_inputs['input_ids'][0, :]        attention_masks = sent_inputs['attention_mask'][0, :]        sequence_len = input_ids.shape[-1]        valid_ids = [1] + valid_ids + [1]        mem_valid_ids = [0] + [1] * (sequence_len - 2)        dep_instance_parser = DepInstanceParser(basicDependencies=depinfo, tokens=[])        dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_first_order()        token_head_list = []        for input_id, valid_id in zip(input_ids, valid_ids):            if input_id == cls_id:                continue            if input_id == sep_id:                break            if valid_id == 1:                token_head_list.append(input_id)        key_list = token_head_list[:self.max_key_len]        if len(key_list) < self.max_key_len:            key_list = key_list + [0] * (self.max_key_len - len(key_list))        # print(key_list)        final_dep_adj_matrix = [[0] * self.max_key_len for _ in range(self.tokenizer.max_seq_len)]        # GCN 进行邻接矩阵的变换        dep_adj_matrix = self.normlized_adj(dep_adj_matrix)        for i in range(len(token_head_list)):            if i >= len(dep_adj_matrix[0]):                break            for j in range(len(dep_adj_matrix[i])):                if j >= self.max_key_len - 1:                    break                final_dep_adj_matrix[i + 1][j + 1] = dep_adj_matrix[i][j]        input_ids = self.tokenizer.id_to_sequence(input_ids)        attention_masks = self.tokenizer.id_to_sequence(attention_masks)        valid_ids = self.tokenizer.id_to_sequence(valid_ids)        mem_valid_ids = self.tokenizer.id_to_sequence(mem_valid_ids)        if is_senta:            return {                "input_ids": torch.tensor(input_ids),                "attention_masks": torch.tensor(attention_masks),                "valid_ids": torch.tensor(valid_ids),                "mem_valid_ids": torch.tensor(mem_valid_ids),                "dep_adj_matrix": torch.tensor(final_dep_adj_matrix),                "sim_score": self.label2id[sim_score.lower()] if task in ['SNLI', 'SICK-E', 'MRPC',                                                                          'SST'] else torch.tensor(sim_score),                "raw_text": sent            }        else:            return {                "input_ids_b": torch.tensor(input_ids),                "attention_masks_b": torch.tensor(attention_masks),                "valid_ids_b": torch.tensor(valid_ids),                "mem_valid_ids_b": torch.tensor(mem_valid_ids),                "dep_adj_matrix_b": torch.tensor(final_dep_adj_matrix),                "sim_score_b": self.label2id[sim_score.lower()] if task in ['SNLI', 'SICK-E', 'MRPC',                                                                            'SST'] else torch.tensor(sim_score),                "raw_text_b": sent            }    @staticmethod    def load_depfile(filename, is_sentence_a):        data = []        line_count = 0        with open(filename, 'r') as f:            dep_info = []            for line in f:                line = line.strip()                if len(line) == 0:  # 空白行                    line_count += 1                if is_sentence_a and line_count % 2 == 1:                    if len(line) > 0:                        items = line.split("\t")                        dep_info.append({                            "governor": int(items[0]),                            "dependent": int(items[1]),                            "dep": items[2],                        })                    else:                        if len(dep_info) > 0:                            data.append(dep_info)                            dep_info = []                elif not is_sentence_a and line_count % 2 == 0:                    if len(line) > 0:                        items = line.split("\t")                        dep_info.append({                            "governor": int(items[0]),                            "dependent": int(items[1]),                            "dep": items[2],                        })                    else:                        if len(dep_info) > 0:                            data.append(dep_info)                            dep_info = []            if len(dep_info) > 0:                data.append(dep_info)        return data    @staticmethod    def load_datafile(filename, task_name, is_sentence_a):        def encode_labels(label, nclass=5):            """            Label encoding from Tree LSTM paper (Tai, Socher, Manning)            """            y = np.zeros(nclass).astype('float32')            # print(y)            for i in range(nclass):                if i + 1 == np.floor(label) + 1:                    y[i] = label - np.floor(label)                if i + 1 == np.floor(label):                    y[i] = np.floor(label) - label + 1            return y        data = []        r = np.arange(1, 6)        with open(filename, 'r', encoding='utf-8') as f:            reader = csv.reader(f, delimiter="\t", quoting=csv.QUOTE_NONE)            for (i, line) in enumerate(reader):                if task_name == 'STS-B':                    if 'test.csv' in filename:                        sentence = line[-2] if is_sentence_a else line[-1]                        label = float(line[-3])                        label = encode_labels(label)                        data.append([sentence, label])                    elif 'test.csv' not in filename and i > 0:                        sentence = line[-3] if is_sentence_a else line[-2]                        label = float(line[-1])                        label = encode_labels(label)                        data.append([sentence, label])                elif task_name == 'SICK-R':                    if i == 0:                        continue                    sentence = line[1] if is_sentence_a else line[2]                    label = float(line[3])                    label = encode_labels(label)                    data.append([sentence, label])                elif task_name == 'SICK-E':                    if i == 0:                        continue                    sentence = line[1] if is_sentence_a else line[2]                    label = line[4]                    data.append([sentence, label])                elif task_name == 'SNLI':                    sentence = line[0] if is_sentence_a else line[1]                    label = line[2]                    data.append([sentence, label])                elif task_name == 'MRPC':                    if i == 0:  # 跳过第一行表头                        continue                    sentence = line[-2] if is_sentence_a else line[-1]                    label = line[0]                    data.append([sentence, label])                elif task_name == 'SST':                    sentence = line[0]                    label = line[1]                    data.append([sentence, label])        return data    @staticmethod    def load_alldepfile(filename):        data = []        with open(filename, 'r') as f:            dep_info = []            for line in f:                line = line.strip()                if len(line) > 0:                    items = line.split("\t")                    dep_info.append({                        "governor": int(items[0]),                        "dependent": int(items[1]),                        "dep": items[2],                    })                else:                    if len(dep_info) > 0:                        data.append(dep_info)                        dep_info = []            if len(dep_info) > 0:                data.append(dep_info)                dep_info = []        return data    @staticmethod    def load_alldatafile(filename, task_name):        data = []        print(filename)        with open(filename, 'r', encoding='utf-8') as f:            reader = csv.reader(f, delimiter="\t", quoting=csv.QUOTE_NONE)            for (i, line) in enumerate(reader):                print(i)                if task_name == 'STS-B':                    print('数据行:', line[-3:-1])                    if 'test.csv' not in filename and i == 0:                        continue                    similarity_soce = float(line[-1])                    data.append([line[-3], similarity_soce])  # [句子A,similarity score]                    data.append([line[-2], similarity_soce])  # [句子B,similarity score]                elif task_name == 'SICK':                    pass        return data    @staticmethod    def load_deptype_map(opt):        deptype_set = set()        for filename in [opt.train_file, opt.test_file, opt.val_file]:            filename = "{}.dep".format(filename)            if os.path.exists(filename) is False:                continue            data = ABSADataset.load_alldepfile(filename)            for dep_info in data:                for item in dep_info:                    deptype_set.add(item['dep'])        deptype_map = {"none": 0}        for deptype in sorted(deptype_set, key=lambda x: x):            deptype_map[deptype] = len(deptype_map)        return deptype_map    @staticmethod    def get_label2id(opt):        # polarity_label = ["-1","0","1"]        label_list = ["neutral", "entailment", "contradiction"]        if opt.task_name in ['MRPC', 'SST']:            label_list = ["0", "1"]        return dict([(label, idx) for idx, label in enumerate(label_list)])